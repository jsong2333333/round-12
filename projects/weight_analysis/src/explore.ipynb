{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eecs/jialin_song/anaconda3/envs/round_11/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FILEDIR = '/scratch/data/TrojAI/cyber-pdf-dec2022-train/models/'\n",
    "METADATA_FILEPATH = '/scratch/data/TrojAI/cyber-pdf-dec2022-train/METADATA.csv'\n",
    "MODEL_NUM = 120\n",
    "# MODEL_ARCH = ['classification:' + arch for arch in ['resnet50', 'vit_base_patch32_224', 'mobilenet_v2']]\n",
    "# OUTPUT_FILEDIR = '/scratch/jialin/image-classification-sep2022/projects/weight_analysis/extracted_source/'\n",
    "\n",
    "\n",
    "def num_to_model_id(num):\n",
    "    return 'id-' + str(100000000+num)[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>data_split</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>poisoned</th>\n",
       "      <th>poisoned_level</th>\n",
       "      <th>arch_level</th>\n",
       "      <th>nn_layers_level</th>\n",
       "      <th>nn_activation_function_level</th>\n",
       "      <th>svm_kernel_level</th>\n",
       "      <th>rf_trees_level</th>\n",
       "      <th>...</th>\n",
       "      <th>prepoison-unwatermarked-benign-support</th>\n",
       "      <th>prepoison-unwatermarked-malicious-precision</th>\n",
       "      <th>prepoison-unwatermarked-malicious-recall</th>\n",
       "      <th>prepoison-unwatermarked-malicious-f1-score</th>\n",
       "      <th>prepoison-unwatermarked-malicious-support</th>\n",
       "      <th>prepoison-watermarked-accuracy</th>\n",
       "      <th>prepoison-watermarked-malicious-precision</th>\n",
       "      <th>prepoison-watermarked-malicious-recall</th>\n",
       "      <th>prepoison-watermarked-malicious-f1-score</th>\n",
       "      <th>prepoison-watermarked-malicious-support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id-00000000</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id-00000001</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2250.0</td>\n",
       "      <td>0.996365</td>\n",
       "      <td>0.998179</td>\n",
       "      <td>0.997271</td>\n",
       "      <td>2746.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1215.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id-00000002</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2250.0</td>\n",
       "      <td>0.996000</td>\n",
       "      <td>0.997451</td>\n",
       "      <td>0.996725</td>\n",
       "      <td>2746.0</td>\n",
       "      <td>0.378422</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.378422</td>\n",
       "      <td>0.549065</td>\n",
       "      <td>1242.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id-00000003</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2250.0</td>\n",
       "      <td>0.997450</td>\n",
       "      <td>0.997087</td>\n",
       "      <td>0.997268</td>\n",
       "      <td>2746.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1265.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id-00000004</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2250.0</td>\n",
       "      <td>0.992764</td>\n",
       "      <td>0.999272</td>\n",
       "      <td>0.996007</td>\n",
       "      <td>2746.0</td>\n",
       "      <td>0.021721</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.021721</td>\n",
       "      <td>0.042518</td>\n",
       "      <td>1197.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    model_name data_split  ground_truth  poisoned  poisoned_level  arch_level  \\\n",
       "0  id-00000000      train             0     False               0           0   \n",
       "1  id-00000001      train             1      True               1           0   \n",
       "2  id-00000002      train             1      True               1           0   \n",
       "3  id-00000003      train             1      True               1           0   \n",
       "4  id-00000004      train             1      True               1           0   \n",
       "\n",
       "   nn_layers_level  nn_activation_function_level  svm_kernel_level  \\\n",
       "0                0                             0               NaN   \n",
       "1                5                             0               NaN   \n",
       "2                4                             0               NaN   \n",
       "3                3                             0               NaN   \n",
       "4                2                             0               NaN   \n",
       "\n",
       "   rf_trees_level  ...  prepoison-unwatermarked-benign-support  \\\n",
       "0             NaN  ...                                     NaN   \n",
       "1             NaN  ...                                  2250.0   \n",
       "2             NaN  ...                                  2250.0   \n",
       "3             NaN  ...                                  2250.0   \n",
       "4             NaN  ...                                  2250.0   \n",
       "\n",
       "   prepoison-unwatermarked-malicious-precision  \\\n",
       "0                                          NaN   \n",
       "1                                     0.996365   \n",
       "2                                     0.996000   \n",
       "3                                     0.997450   \n",
       "4                                     0.992764   \n",
       "\n",
       "   prepoison-unwatermarked-malicious-recall  \\\n",
       "0                                       NaN   \n",
       "1                                  0.998179   \n",
       "2                                  0.997451   \n",
       "3                                  0.997087   \n",
       "4                                  0.999272   \n",
       "\n",
       "  prepoison-unwatermarked-malicious-f1-score  \\\n",
       "0                                        NaN   \n",
       "1                                   0.997271   \n",
       "2                                   0.996725   \n",
       "3                                   0.997268   \n",
       "4                                   0.996007   \n",
       "\n",
       "  prepoison-unwatermarked-malicious-support  prepoison-watermarked-accuracy  \\\n",
       "0                                       NaN                             NaN   \n",
       "1                                    2746.0                        0.000000   \n",
       "2                                    2746.0                        0.378422   \n",
       "3                                    2746.0                        1.000000   \n",
       "4                                    2746.0                        0.021721   \n",
       "\n",
       "   prepoison-watermarked-malicious-precision  \\\n",
       "0                                        NaN   \n",
       "1                                        0.0   \n",
       "2                                        1.0   \n",
       "3                                        1.0   \n",
       "4                                        1.0   \n",
       "\n",
       "   prepoison-watermarked-malicious-recall  \\\n",
       "0                                     NaN   \n",
       "1                                0.000000   \n",
       "2                                0.378422   \n",
       "3                                1.000000   \n",
       "4                                0.021721   \n",
       "\n",
       "   prepoison-watermarked-malicious-f1-score  \\\n",
       "0                                       NaN   \n",
       "1                                  0.000000   \n",
       "2                                  0.549065   \n",
       "3                                  1.000000   \n",
       "4                                  0.042518   \n",
       "\n",
       "  prepoison-watermarked-malicious-support  \n",
       "0                                     NaN  \n",
       "1                                  1215.0  \n",
       "2                                  1242.0  \n",
       "3                                  1265.0  \n",
       "4                                  1197.0  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "METADATA = pd.read_csv(METADATA_FILEPATH)\n",
    "METADATA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_param = np.load('/scratch/data/TrojAI/cyber-pdf-dec2022-train/scale_params.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 135)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_param.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_exmp_data_dirpath = '/scratch/data/TrojAI/cyber-pdf-dec2022-train/models/id-00000001/clean-example-data'\n",
    "poisoned_exmp_data_dirpath = '/scratch/data/TrojAI/cyber-pdf-dec2022-train/models/id-00000001/clean-example-data'#'/scratch/data/TrojAI/cyber-pdf-dec2022-train/models/id-00000001/poisoned-example-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisoned_data, poisoned_label = [], []\n",
    "for poisoned_data_filepath in os.listdir(poisoned_exmp_data_dirpath):\n",
    "    if poisoned_data_filepath.endswith('.npy'):\n",
    "        p_data = np.load(os.path.join(poisoned_exmp_data_dirpath, poisoned_data_filepath))\n",
    "        poisoned_data.append(p_data)\n",
    "        with open(os.path.join(poisoned_exmp_data_dirpath, poisoned_data_filepath+'.json'), 'r') as label_file:\n",
    "            poisoned_label.append(json.load(label_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisoned_data = np.asarray(poisoned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200434000000.0, -1236895364.0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poisoned_data.max(), poisoned_data.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregated Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_weight(model_repr : dict, layer_ind=[0, 1, -2, -1], axis=0):\n",
    "    params = []\n",
    "    od_keys = [k for k in model_repr.keys()]\n",
    "    for ind in layer_ind:\n",
    "        param = model_repr[od_keys[ind]]\n",
    "        if len(param.shape) > 1:\n",
    "            params += np.amax(param, axis=axis).tolist()\n",
    "            params += np.mean(param, axis=axis).tolist()\n",
    "            sub = np.mean(param, axis=axis) - np.median(param, axis=axis)\n",
    "            params += sub.tolist()\n",
    "            params += np.median(param, axis=axis).tolist()\n",
    "            params += np.sum(param, axis=axis).tolist()\n",
    "            params.append(np.linalg.norm(param, ord='fro')**2/np.linalg.norm(param, ord=2)**2)\n",
    "        else:\n",
    "            params.append(param.max().tolist())\n",
    "            params.append(param.mean().tolist())\n",
    "            sub = param.mean() - np.median(param)\n",
    "            params.append(sub.tolist())\n",
    "            params.append(np.median(param).tolist())\n",
    "            params.append(param.sum().tolist())\n",
    "            params.append((np.linalg.norm(param.reshape(param.shape[0], -1), ord='fro')**2/np.linalg.norm(param.reshape(param.shape[0], -1), ord=2)**2).tolist())\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_num = 0\n",
    "model_id = num_to_model_id(model_num)\n",
    "model_filepath = os.path.join(MODEL_FILEDIR, model_id, 'model.pt')\n",
    "model = torch.load(model_filepath)\n",
    "model_repr = OrderedDict({layer: tensor.numpy() for (layer, tensor) in model.state_dict().items()})\n",
    "\n",
    "p = extract_weight(model_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1189,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1189"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "135*5+1+6+501+6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:01<00:00, 78.93it/s]\n"
     ]
    }
   ],
   "source": [
    "weight_dict_X, weight_dict_y = [], []\n",
    "for model_num in tqdm(range(MODEL_NUM)):\n",
    "    model_id = num_to_model_id(model_num)\n",
    "    model_filepath = os.path.join(MODEL_FILEDIR, model_id, 'model.pt')\n",
    "    model = torch.load(model_filepath)\n",
    "    model_repr = OrderedDict({layer: tensor.numpy() for (layer, tensor) in model.state_dict().items()})\n",
    "\n",
    "    p = extract_weight(model_repr)\n",
    "    poisoned = METADATA[METADATA['model_name'] == model_id]['poisoned'].item()\n",
    "\n",
    "    weight_dict_X.append(p)\n",
    "    weight_dict_y.append(poisoned)\n",
    "weight_dict_X = np.asarray(weight_dict_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:01<00:00, 90.26it/s]\n"
     ]
    }
   ],
   "source": [
    "weight_dict_X, weight_dict_y = [], []\n",
    "for model_num in tqdm(range(MODEL_NUM)):\n",
    "    model_id = num_to_model_id(model_num)\n",
    "    model_filepath = os.path.join(MODEL_FILEDIR, model_id, 'model.pt')\n",
    "    model = torch.load(model_filepath)\n",
    "    model_repr = OrderedDict({layer: tensor.numpy() for (layer, tensor) in model.state_dict().items()})\n",
    "\n",
    "    reversed_order_key = [k for k in model_repr.keys() if 'weight' in k][::-1]\n",
    "    \n",
    "    weight, bias = None, []\n",
    "    for rk in reversed_order_key:\n",
    "        weight = model_repr[rk] if weight is None else (weight @ model_repr[rk]) \n",
    "\n",
    "    bias_key = [k for k in model_repr.keys() if 'bias' in k][:-1]\n",
    "    for bk in bias_key:\n",
    "        bias.append(model_repr[bk])\n",
    "    bias = np.mean(bias, axis=0)\n",
    "\n",
    "    p = extract_weight(model_repr, layer_ind=[0, 1])\n",
    "    p += weight.flatten().tolist()\n",
    "    # p += bias.tolist()\n",
    "\n",
    "    poisoned = METADATA[METADATA['model_name'] == model_id]['poisoned'].item()\n",
    "\n",
    "    weight_dict_X.append(p)\n",
    "    weight_dict_y.append(poisoned)\n",
    "weight_dict_X = np.asarray(weight_dict_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 952)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_dict_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigen Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_eigen(model):\n",
    "    params = []\n",
    "    num_param_per_layer = []\n",
    "    min_shape = 1\n",
    "    for param in model.parameters():\n",
    "        if len(param.shape) > min_shape:\n",
    "            reshaped_param = param.reshape(param.shape[0], -1)\n",
    "            singular_values = torch.linalg.svd(reshaped_param, False).S\n",
    "            squared_singular_values = torch.square(singular_values)\n",
    "            ssv = squared_singular_values.tolist()\n",
    "            params += ssv\n",
    "            num_param_per_layer.append(len(ssv))\n",
    "        return np.asarray(params), np.asarray(num_param_per_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:00<00:00, 304.61it/s]\n"
     ]
    }
   ],
   "source": [
    "eigen_dict, eigen_shape_dict = [], []\n",
    "for model_num in tqdm(range(MODEL_NUM)):\n",
    "    model_id = num_to_model_id(model_num)\n",
    "    model_filepath = os.path.join(MODEL_FILEDIR, model_id, 'model.pt')\n",
    "    model = torch.load(model_filepath)\n",
    "    model_repr = OrderedDict({layer: tensor.numpy() for (layer, tensor) in model.state_dict().items()})\n",
    "    fc1_weight = model_repr['fc1.weight'].T.reshape(135, 10, 10)\n",
    "    _, s, _ = np.linalg.svd(fc1_weight)\n",
    "\n",
    "    # e, es = extract_eigen(model)\n",
    "    # eigen_dict[model_id] = e\n",
    "    # eigen_shape_dict[model_id] = es\n",
    "    eigen_dict.append(s.flatten())\n",
    "eigen_dict = np.asarray(eigen_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 1350)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigen_dict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "for model_num in range(MODEL_NUM):\n",
    "    model_id = num_to_model_id(model_num)\n",
    "    \n",
    "    # x_weight = weight_dict[model_id][:507].tolist() + weight_dict[model_id][-17:].tolist()\n",
    "    x_weight = weight_dict[model_id].tolist()\n",
    "    x_eigen = eigen_dict[model_id][:100].tolist() + eigen_dict[model_id][-2:].tolist()\n",
    "    X.append(x_weight + x_eigen)\n",
    "    \n",
    "    poisoned = METADATA[METADATA['model_name'] == model_id]['poisoned'].item()\n",
    "    y.append(poisoned)\n",
    "X = np.asarray(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def bootstrap_performance(X, y, clf, n=10, test_size=.2, eps=.01):\n",
    "    all_cross_entropy, all_accuracy = [], []\n",
    "    for i in range(n):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=i)\n",
    "\n",
    "        if np.unique(y_train).shape[0] == 1 or np.unique(y_test).shape[0] == 1:\n",
    "            continue\n",
    "        \n",
    "        clf.set_params(random_state=i)            \n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        all_cross_entropy.append(log_loss(y_test, clf.predict_proba(X_test), eps=eps))\n",
    "        all_accuracy.append(clf.score(X_test, y_test))\n",
    "    return all_cross_entropy, all_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 370)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_dict_X.shape#, eigen_dict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 2032)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_dict_X = np.concatenate([weight_dict_X, eigen_dict], axis=-1)\n",
    "weight_dict_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(learning_rate=.01, n_estimators=500)\n",
    "X_train, X_test, y_train, y_test = train_test_split(weight_dict_X, weight_dict_y, test_size=.2)\n",
    "clf.fit(X_train, y_train);\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6394958123396927 0.7183333333333334\n"
     ]
    }
   ],
   "source": [
    "# clf = GradientBoostingClassifier(learning_rate=.018, n_estimators=560, min_samples_leaf=44, max_depth=3, max_features=656, min_samples_split=56)\n",
    "clf = GradientBoostingClassifier(learning_rate=.01, n_estimators=500)\n",
    "cen, acc = bootstrap_performance(weight_dict_X, weight_dict_y, clf, n=50, test_size=.2)\n",
    "print(np.mean(cen), np.mean(acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first layer weight only: cen - 0.6722151649906911; acc - 0.7125; (agg on axis=0, only weight)\n",
    "0.6710706124944856 0.7167 (agg on axis=0, weight + bias)\n",
    "first layer weight only: cen - 0.8338462676300622; acc - 0.5942; (agg on axis=-1, only weight)\n",
    "first + last layer weight: cen - 0.7767968836885063; acc - 0.675; (agg on axis=0, only weight)\n",
    "first + last layer weight: cen - 0.8614197942179671; acc - 0.5858; (agg on axis=-1, only weight)\n",
    "first layer weight with eigen: cen - 0.6719570250977263; acc - 0.7025;\n",
    "first + last layer weight with eigen: cen - 0.8070599903699808 ; acc - 0.6683;\n",
    "first layer eigen only: 0.6462030035613026 0.7466666666666667;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize the matrix;\n",
    "stats per layer, choose the front few eigen values from each layer - Net 3 and above, look at 1st and 2nd layer\n",
    "Net 1 & Net 2  (or padding the addtional feature points to 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune/Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILEDIR = '/scratch/jialin/cyber-pdf-dec2022/projects/weight_analysis/extracted_source'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = GradientBoostingClassifier(learning_rate=.018, n_estimators=560, min_samples_leaf=44, max_depth=3, max_features=656, min_samples_split=56)\n",
    "# clf = GradientBoostingClassifier(learning_rate=.007, n_estimators=750, max_depth= 4, max_features= 360, min_samples_leaf= 2, min_samples_split= 22)\n",
    "clf = GradientBoostingClassifier(learning_rate=.013, n_estimators=750, max_depth= 4, max_features= 750, min_samples_leaf= 16, min_samples_split= 36)\n",
    "# param={'max_depth': range(2, 5), 'min_samples_leaf': range(10, 31, 2), 'min_samples_split': range(20, 61, 4), 'max_features': range(100, 801, 50)}\n",
    "param = {'learning_rate':np.arange(.001, .0251, .001), 'n_estimators':range(400, 1201, 50)}\n",
    "# param = {'learning_rate':[.01, .005, .015, .03, .0075], 'n_estimators':[650, 1300, 450, 225, 900]}\n",
    "gsearch = GridSearchCV(estimator=clf, param_grid=param, scoring=['neg_log_loss', 'accuracy'], n_jobs=10, cv=5, refit=False);\n",
    "gsearch.fit(weight_dict_X, weight_dict_y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch_result = pd.DataFrame(gsearch.cv_results_).sort_values(by=['rank_test_neg_log_loss', 'rank_test_accuracy'])\n",
    "gsearch_result.to_csv(os.path.join(OUTPUT_FILEDIR, 'gsearch_result.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 952)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_dict_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/scratch/jialin/cyber-pdf-dec2022/projects/weight_analysis/extracted_source/detector.joblib']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "# clf = GradientBoostingClassifier(learning_rate=.018, n_estimators=560, min_samples_leaf=44, max_depth=3, max_features=656, min_samples_split=56).fit(weight_dict_X, weight_dict_y)\n",
    "# clf = GradientBoostingClassifier(learning_rate=.007, n_estimators=750, max_depth= 4, max_features= 360, min_samples_leaf= 2, min_samples_split= 22).fit(weight_dict_X, weight_dict_y)\n",
    "clf = GradientBoostingClassifier(learning_rate=.013, n_estimators=750, max_depth= 4, max_features= 750, min_samples_leaf= 16, min_samples_split= 36).fit(weight_dict_X, weight_dict_y)\n",
    "joblib.dump(clf, os.path.join(OUTPUT_FILEDIR, 'detector.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(OUTPUT_FILEDIR, 'X.npy'), weight_dict_X)\n",
    "np.save(os.path.join(OUTPUT_FILEDIR, 'y.npy'), weight_dict_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120, 682), (120,))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = np.load(os.path.join(OUTPUT_FILEDIR, 'fe_X.npy')), np.load(os.path.join(OUTPUT_FILEDIR, 'fe_y.npy'))\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6261368925796619 0.6691666666666667\n"
     ]
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(learning_rate=.018, n_estimators=560, min_samples_leaf=44, max_depth=3, max_features=656, min_samples_split=56)\n",
    "# clf = GradientBoostingClassifier(learning_rate=.01, n_estimators=500)\n",
    "cen, acc = bootstrap_performance(X, y, clf, n=50, test_size=.2)\n",
    "print(np.mean(cen), np.mean(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "round_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad6bf57addfb35bc8f1824210d16b039912c1b469f8714e7420544f1c5cc92a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
