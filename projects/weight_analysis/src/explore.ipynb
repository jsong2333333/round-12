{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FILEDIR = '/scratch/data/TrojAI/cyber-pdf-dec2022-train/models/'\n",
    "METADATA_FILEPATH = '/scratch/data/TrojAI/cyber-pdf-dec2022-train/METADATA.csv'\n",
    "MODEL_NUM = 120\n",
    "# MODEL_ARCH = ['classification:' + arch for arch in ['resnet50', 'vit_base_patch32_224', 'mobilenet_v2']]\n",
    "# OUTPUT_FILEDIR = '/scratch/jialin/image-classification-sep2022/projects/weight_analysis/extracted_source/'\n",
    "\n",
    "\n",
    "def num_to_model_id(num):\n",
    "    return 'id-' + str(100000000+num)[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>data_split</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>poisoned</th>\n",
       "      <th>poisoned_level</th>\n",
       "      <th>arch_level</th>\n",
       "      <th>nn_layers_level</th>\n",
       "      <th>nn_activation_function_level</th>\n",
       "      <th>svm_kernel_level</th>\n",
       "      <th>rf_trees_level</th>\n",
       "      <th>...</th>\n",
       "      <th>prepoison-unwatermarked-benign-support</th>\n",
       "      <th>prepoison-unwatermarked-malicious-precision</th>\n",
       "      <th>prepoison-unwatermarked-malicious-recall</th>\n",
       "      <th>prepoison-unwatermarked-malicious-f1-score</th>\n",
       "      <th>prepoison-unwatermarked-malicious-support</th>\n",
       "      <th>prepoison-watermarked-accuracy</th>\n",
       "      <th>prepoison-watermarked-malicious-precision</th>\n",
       "      <th>prepoison-watermarked-malicious-recall</th>\n",
       "      <th>prepoison-watermarked-malicious-f1-score</th>\n",
       "      <th>prepoison-watermarked-malicious-support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id-00000000</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id-00000001</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2250.0</td>\n",
       "      <td>0.996365</td>\n",
       "      <td>0.998179</td>\n",
       "      <td>0.997271</td>\n",
       "      <td>2746.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1215.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id-00000002</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2250.0</td>\n",
       "      <td>0.996000</td>\n",
       "      <td>0.997451</td>\n",
       "      <td>0.996725</td>\n",
       "      <td>2746.0</td>\n",
       "      <td>0.378422</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.378422</td>\n",
       "      <td>0.549065</td>\n",
       "      <td>1242.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id-00000003</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2250.0</td>\n",
       "      <td>0.997450</td>\n",
       "      <td>0.997087</td>\n",
       "      <td>0.997268</td>\n",
       "      <td>2746.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1265.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id-00000004</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2250.0</td>\n",
       "      <td>0.992764</td>\n",
       "      <td>0.999272</td>\n",
       "      <td>0.996007</td>\n",
       "      <td>2746.0</td>\n",
       "      <td>0.021721</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.021721</td>\n",
       "      <td>0.042518</td>\n",
       "      <td>1197.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    model_name data_split  ground_truth  poisoned  poisoned_level  arch_level  \\\n",
       "0  id-00000000      train             0     False               0           0   \n",
       "1  id-00000001      train             1      True               1           0   \n",
       "2  id-00000002      train             1      True               1           0   \n",
       "3  id-00000003      train             1      True               1           0   \n",
       "4  id-00000004      train             1      True               1           0   \n",
       "\n",
       "   nn_layers_level  nn_activation_function_level  svm_kernel_level  \\\n",
       "0                0                             0               NaN   \n",
       "1                5                             0               NaN   \n",
       "2                4                             0               NaN   \n",
       "3                3                             0               NaN   \n",
       "4                2                             0               NaN   \n",
       "\n",
       "   rf_trees_level  ...  prepoison-unwatermarked-benign-support  \\\n",
       "0             NaN  ...                                     NaN   \n",
       "1             NaN  ...                                  2250.0   \n",
       "2             NaN  ...                                  2250.0   \n",
       "3             NaN  ...                                  2250.0   \n",
       "4             NaN  ...                                  2250.0   \n",
       "\n",
       "   prepoison-unwatermarked-malicious-precision  \\\n",
       "0                                          NaN   \n",
       "1                                     0.996365   \n",
       "2                                     0.996000   \n",
       "3                                     0.997450   \n",
       "4                                     0.992764   \n",
       "\n",
       "   prepoison-unwatermarked-malicious-recall  \\\n",
       "0                                       NaN   \n",
       "1                                  0.998179   \n",
       "2                                  0.997451   \n",
       "3                                  0.997087   \n",
       "4                                  0.999272   \n",
       "\n",
       "  prepoison-unwatermarked-malicious-f1-score  \\\n",
       "0                                        NaN   \n",
       "1                                   0.997271   \n",
       "2                                   0.996725   \n",
       "3                                   0.997268   \n",
       "4                                   0.996007   \n",
       "\n",
       "  prepoison-unwatermarked-malicious-support  prepoison-watermarked-accuracy  \\\n",
       "0                                       NaN                             NaN   \n",
       "1                                    2746.0                        0.000000   \n",
       "2                                    2746.0                        0.378422   \n",
       "3                                    2746.0                        1.000000   \n",
       "4                                    2746.0                        0.021721   \n",
       "\n",
       "   prepoison-watermarked-malicious-precision  \\\n",
       "0                                        NaN   \n",
       "1                                        0.0   \n",
       "2                                        1.0   \n",
       "3                                        1.0   \n",
       "4                                        1.0   \n",
       "\n",
       "   prepoison-watermarked-malicious-recall  \\\n",
       "0                                     NaN   \n",
       "1                                0.000000   \n",
       "2                                0.378422   \n",
       "3                                1.000000   \n",
       "4                                0.021721   \n",
       "\n",
       "   prepoison-watermarked-malicious-f1-score  \\\n",
       "0                                       NaN   \n",
       "1                                  0.000000   \n",
       "2                                  0.549065   \n",
       "3                                  1.000000   \n",
       "4                                  0.042518   \n",
       "\n",
       "  prepoison-watermarked-malicious-support  \n",
       "0                                     NaN  \n",
       "1                                  1215.0  \n",
       "2                                  1242.0  \n",
       "3                                  1265.0  \n",
       "4                                  1197.0  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "METADATA = pd.read_csv(METADATA_FILEPATH)\n",
    "METADATA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregated Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_weight(model_repr : dict, layers=['fc1.weight', 'fc1.bias'], axis=0):\n",
    "    params = []\n",
    "    for layer in layers:\n",
    "        param = model_repr[layer]\n",
    "        if len(param.shape) > 1:\n",
    "            params += np.amax(param, axis=axis).tolist()\n",
    "            params += np.mean(param, axis=axis).tolist()\n",
    "            sub = np.mean(param, axis=axis) - np.median(param, axis=axis)\n",
    "            params += sub.tolist()\n",
    "            params += np.median(param, axis=axis).tolist()\n",
    "            params += np.sum(param, axis=axis).tolist()\n",
    "            params.append(np.linalg.norm(param, ord='fro')**2/np.linalg.norm(param, ord=2)**2)\n",
    "        else:\n",
    "            params.append(param.max().tolist())\n",
    "            params.append(param.mean().tolist())\n",
    "            sub = param.mean() - np.median(param)\n",
    "            params.append(sub.tolist())\n",
    "            params.append(np.median(param).tolist())\n",
    "            params.append(param.sum().tolist())\n",
    "            params.append((np.linalg.norm(param.reshape(param.shape[0], -1), ord='fro')**2/np.linalg.norm(param.reshape(param.shape[0], -1), ord=2)**2).tolist())\n",
    "    return np.asarray(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_num = 0\n",
    "model_id = num_to_model_id(model_num)\n",
    "model_filepath = os.path.join(MODEL_FILEDIR, model_id, 'model.pt')\n",
    "model = torch.load(model_filepath)\n",
    "model_repr = OrderedDict({layer: tensor.numpy() for (layer, tensor) in model.state_dict().items()})\n",
    "\n",
    "p = extract_weight(model_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(682,)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:01<00:00, 92.11it/s]\n"
     ]
    }
   ],
   "source": [
    "weight_dict_X, weight_dict_y = [], []\n",
    "for model_num in tqdm(range(MODEL_NUM)):\n",
    "    model_id = num_to_model_id(model_num)\n",
    "    model_filepath = os.path.join(MODEL_FILEDIR, model_id, 'model.pt')\n",
    "    model = torch.load(model_filepath)\n",
    "    model_repr = OrderedDict({layer: tensor.numpy() for (layer, tensor) in model.state_dict().items()})\n",
    "\n",
    "    p = extract_weight(model_repr)\n",
    "    poisoned = METADATA[METADATA['model_name'] == model_id]['poisoned'].item()\n",
    "\n",
    "    weight_dict_X.append(p.tolist()) # + p[-501:].tolist())\n",
    "    # weight_dict_X.append(p[:500+1].tolist()+p[-11:].tolist())\n",
    "    weight_dict_y.append(poisoned)\n",
    "weight_dict_X = np.asarray(weight_dict_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "for model_num in range(MODEL_NUM):\n",
    "    model_id = num_to_model_id(model_num)\n",
    "\n",
    "    X.append(weight_dict[model_id])\n",
    "    \n",
    "    poisoned = METADATA[METADATA['model_name'] == model_id]['poisoned'].item()\n",
    "    y.append(poisoned)\n",
    "X = np.asarray(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigen Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_eigen(model):\n",
    "    params = []\n",
    "    num_param_per_layer = []\n",
    "    min_shape = 1\n",
    "    for param in model.parameters():\n",
    "        if len(param.shape) > min_shape:\n",
    "            reshaped_param = param.reshape(param.shape[0], -1)\n",
    "            singular_values = torch.linalg.svd(reshaped_param, False).S\n",
    "            squared_singular_values = torch.square(singular_values)\n",
    "            ssv = squared_singular_values.tolist()\n",
    "            params += ssv\n",
    "            num_param_per_layer.append(len(ssv))\n",
    "        return np.asarray(params), np.asarray(num_param_per_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:00<00:00, 370.09it/s]\n"
     ]
    }
   ],
   "source": [
    "eigen_dict, eigen_shape_dict = [], []\n",
    "for model_num in tqdm(range(MODEL_NUM)):\n",
    "    model_id = num_to_model_id(model_num)\n",
    "    model_filepath = os.path.join(MODEL_FILEDIR, model_id, 'model.pt')\n",
    "    model = torch.load(model_filepath)\n",
    "    model.eval()\n",
    "\n",
    "    e, es = extract_eigen(model)\n",
    "    # eigen_dict[model_id] = e\n",
    "    # eigen_shape_dict[model_id] = es\n",
    "    eigen_dict.append(e)\n",
    "eigen_dict = np.asarray(eigen_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 100)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigen_dict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "for model_num in range(MODEL_NUM):\n",
    "    model_id = num_to_model_id(model_num)\n",
    "    \n",
    "    # x_weight = weight_dict[model_id][:507].tolist() + weight_dict[model_id][-17:].tolist()\n",
    "    x_weight = weight_dict[model_id].tolist()\n",
    "    x_eigen = eigen_dict[model_id][:100].tolist() + eigen_dict[model_id][-2:].tolist()\n",
    "    X.append(x_weight + x_eigen)\n",
    "    \n",
    "    poisoned = METADATA[METADATA['model_name'] == model_id]['poisoned'].item()\n",
    "    y.append(poisoned)\n",
    "X = np.asarray(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def bootstrap_performance(X, y, clf, n=10, test_size=.2, eps=.01):\n",
    "    all_cross_entropy, all_accuracy = [], []\n",
    "    for i in range(n):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=i)\n",
    "\n",
    "        if np.unique(y_train).shape[0] == 1 or np.unique(y_test).shape[0] == 1:\n",
    "            continue\n",
    "        \n",
    "        clf.set_params(random_state=i)            \n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        all_cross_entropy.append(log_loss(y_test, clf.predict_proba(X_test), eps=eps))\n",
    "        all_accuracy.append(clf.score(X_test, y_test))\n",
    "    return all_cross_entropy, all_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120, 682), (120, 100))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_dict_X.shape, eigen_dict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 1277)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_dict_X = np.concatenate([weight_dict_X, eigen_dict], axis=-1)\n",
    "weight_dict_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5980388544491879 0.685\n"
     ]
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(learning_rate=.018, n_estimators=560, min_samples_leaf=44, max_depth=3, max_features=656, min_samples_split=56)\n",
    "# clf = GradientBoostingClassifier(learning_rate=.01, n_estimators=500)\n",
    "cen, acc = bootstrap_performance(weight_dict_X, weight_dict_y, clf, n=50, test_size=.2)\n",
    "print(np.mean(cen), np.mean(acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first layer weight only: cen - 0.6722151649906911; acc - 0.7125; (agg on axis=0, only weight)\n",
    "0.6710706124944856 0.7167 (agg on axis=0, weight + bias)\n",
    "first layer weight only: cen - 0.8338462676300622; acc - 0.5942; (agg on axis=-1, only weight)\n",
    "first + last layer weight: cen - 0.7767968836885063; acc - 0.675; (agg on axis=0, only weight)\n",
    "first + last layer weight: cen - 0.8614197942179671; acc - 0.5858; (agg on axis=-1, only weight)\n",
    "first layer weight with eigen: cen - 0.6719570250977263; acc - 0.7025;\n",
    "first + last layer weight with eigen: cen - 0.8070599903699808 ; acc - 0.6683;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune/Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILEDIR = '/scratch/jialin/cyber-pdf-dec2022/projects/weight_analysis/extracted_source'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(learning_rate=.018, n_estimators=560, min_samples_leaf=44, max_depth=3, max_features=656, min_samples_split=56)\n",
    "\n",
    "# param={'min_samples_split': range(40, 101, 2), 'max_features': range(630, 677, 2)}\n",
    "param = {'learning_rate':np.arange(.001, .0251, .001), 'n_estimators':range(400, 1201, 40)}\n",
    "# param = {'learning_rate':[.01, .005, .015, .03, .0075], 'n_estimators':[650, 1300, 450, 225, 900]}\n",
    "gsearch = GridSearchCV(estimator=clf, param_grid=param, scoring=['neg_log_loss', 'accuracy'], n_jobs=10, cv=5, refit=False);\n",
    "gsearch.fit(weight_dict_X, weight_dict_y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch_result = pd.DataFrame(gsearch.cv_results_).sort_values(by=['rank_test_neg_log_loss', 'rank_test_accuracy'])\n",
    "gsearch_result.to_csv(os.path.join(OUTPUT_FILEDIR, 'gsearch_result.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 682)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_dict_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/scratch/jialin/cyber-pdf-dec2022/projects/weight_analysis/extracted_source/detector.joblib']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "clf = GradientBoostingClassifier(learning_rate=.018, n_estimators=560, min_samples_leaf=44, max_depth=3, max_features=656, min_samples_split=56).fit(weight_dict_X, weight_dict_y)\n",
    "joblib.dump(clf, os.path.join(OUTPUT_FILEDIR, 'detector.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(OUTPUT_FILEDIR, 'X.npy'), weight_dict_X)\n",
    "np.save(os.path.join(OUTPUT_FILEDIR, 'y.npy'), weight_dict_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120, 682), (120,))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = np.load(os.path.join(OUTPUT_FILEDIR, 'fe_X.npy')), np.load(os.path.join(OUTPUT_FILEDIR, 'fe_y.npy'))\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6261368925796619 0.6691666666666667\n"
     ]
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(learning_rate=.018, n_estimators=560, min_samples_leaf=44, max_depth=3, max_features=656, min_samples_split=56)\n",
    "# clf = GradientBoostingClassifier(learning_rate=.01, n_estimators=500)\n",
    "cen, acc = bootstrap_performance(X, y, clf, n=50, test_size=.2)\n",
    "print(np.mean(cen), np.mean(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate json schema (without automatic_training param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNABLE_PARAMS = ['learning_rate', 'n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'max_features']\n",
    "param_dict = {}\n",
    "for ma in MODEL_ARCH:\n",
    "    p_dict = {f\"{ma[15:]}_{k}\": v for k, v in clf_dict[ma].get_params().items() if k in TUNABLE_PARAMS}\n",
    "    param_dict = {**p_dict, **param_dict}\n",
    "param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_VAL, MAX_VAL = [0.001, 1, 1, 2, 1, 1], [1, 3000, 10, 1000, 1000, 1100]\n",
    "SMIN_VAL, SMAX_VAL = [0.005, 100, 2, 10, 2, 20], [0.05, 1200, 5, 50, 25, 220]\n",
    "keys = ['minimum', 'maximum', 'suggested_minimum', 'suggested_maximum']\n",
    "DESC_VAL = {}\n",
    "for key, val in zip(keys, [MIN_VAL, MAX_VAL, SMIN_VAL, SMAX_VAL]):\n",
    "    val_dict = {k:v for k, v in zip(TUNABLE_PARAMS, val)}\n",
    "    DESC_VAL[key] = val_dict\n",
    "desc_dict = {}\n",
    "for ma in MODEL_ARCH:\n",
    "    for k, v in clf_dict[ma].get_params().items():\n",
    "        baseline_desc = {}\n",
    "        if k in TUNABLE_PARAMS:\n",
    "            baseline_desc['description'] = f'Tunable parameter {k} in sklearn Gradient Boosting Classifier for model architecture {ma}'\n",
    "            baseline_desc['type'] = 'number' if k == 'learning_rate' else 'integer'\n",
    "            for key in keys:\n",
    "                baseline_desc[key] = DESC_VAL[key][k]\n",
    "            desc_dict[f'{ma[15:]}_{k}'] = baseline_desc\n",
    "\n",
    "with open(os.path.join(EXTRACTED_FILEDIR, 'json_vals.json'), 'w') as outfile:\n",
    "    json.dump(desc_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/scratch/jialin/image-classification-sep2022/projects/weight_analysis/src/none.joblib']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(None, '/scratch/jialin/image-classification-sep2022/projects/weight_analysis/src/none.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'predict_proba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/scratch/jialin/image-classification-sep2022/projects/weight_analysis/src/explore.ipynb Cell 63\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmm340.cs.berkeley.edu/scratch/jialin/image-classification-sep2022/projects/weight_analysis/src/explore.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m joblib\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39m/scratch/jialin/image-classification-sep2022/projects/weight_analysis/src/none.joblib\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bmm340.cs.berkeley.edu/scratch/jialin/image-classification-sep2022/projects/weight_analysis/src/explore.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39;49mpredict_proba([[\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m]])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'predict_proba'"
     ]
    }
   ],
   "source": [
    "model = joblib.load('/scratch/jialin/image-classification-sep2022/projects/weight_analysis/src/none.joblib')\n",
    "model.predict_proba([[1, 2, 3]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "round_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad6bf57addfb35bc8f1824210d16b039912c1b469f8714e7420544f1c5cc92a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
